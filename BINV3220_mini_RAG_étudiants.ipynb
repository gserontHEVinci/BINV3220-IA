{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwMHWVMSRLKZZ1cKm2WCnU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gserontHEVinci/BINV3220-IA/blob/main/BINV3220_mini_RAG_%C3%A9tudiants.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Dans cette fiche, nous allons développer un système **RAG** (*Retrieval Augmented Generation*).  \n",
        "L’objectif est de comprendre les différentes étapes nécessaires pour enrichir les capacités d’un modèle de langage en lui donnant accès à une base de connaissances externe.\n",
        "\n",
        "Nous allons procéder **progressivement** :\n",
        "\n",
        "1. **Tester le modèle d’embedding** :  \n",
        "   Nous commencerons par construire des vecteurs de représentation (*embeddings*) à partir de quelques phrases simples, afin de vérifier que le modèle d’embedding fonctionne correctement.\n",
        "\n",
        "2. **Stockage et recherche vectorielle** :  \n",
        "   Nous utiliserons ensuite une base de données vectorielle pour stocker ces embeddings. Cela nous permettra de retrouver les phrases les plus proches d’une requête donnée grâce à une recherche par similarité.\n",
        "\n",
        "3. **Mise en place du système RAG complet** :  \n",
        "   Enfin, nous combinerons ces éléments pour construire un système qui :  \n",
        "   - enrichit un *prompt* utilisateur avec les passages les plus pertinents retrouvés,  \n",
        "   - envoie ce *prompt enrichi* à un modèle de langage,  \n",
        "   - obtient une réponse générée à partir de ce contexte.\n",
        "\n",
        "À la fin de cette fiche, vous aurez donc réalisé un prototype de RAG en plusieurs étapes, en comprenant le rôle de chaque composant : **embedding → recherche vectorielle → génération augmentée**.\n"
      ],
      "metadata": {
        "id": "EYvwO_pEVP2c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation, import et chargement du modèle d'embedding\n",
        "\n",
        "Dans cette première cellule, nous allons instancier le modèle d’embedding.  \n",
        "Ce modèle est disponible sur le repository Hugging Face à l’adresse suivante :  \n",
        "[https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).  \n",
        "\n",
        "Nous utiliserons le modèle **all-MiniLM-L6-v2**.  \n",
        "Allez sur le lien indiqué, consultez la documentation, installez la librairie *Sentence Transformers*, puis instanciez ce modèle.\n"
      ],
      "metadata": {
        "id": "ccp3iPOxsm6w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRZnDFq0rjyL"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test de l'embedding\n",
        "\n",
        "Dans cette cellule, à partir des phrases stockées dans la variable `sentences`, complétez le code pour calculer les embeddings de chacune d’entre elles, puis affichez les résultats.\n"
      ],
      "metadata": {
        "id": "dA6XwyLPuQ9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Quelques phrases pour tester\n",
        "sentences = [\n",
        "    \"L'intelligence artificielle transforme l'éducation.\",\n",
        "    \"Les chats adorent dormir au soleil.\",\n",
        "    \"Python est un langage de programmation populaire.\"\n",
        "]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1270e10-d0c3-425c-eb42-33cfa5a50438",
        "id": "NuVEy-R1ea7r"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase: L'intelligence artificielle transforme l'éducation.\n",
            "Embedding (premiers 5 coefficients): [-0.00492159  0.08406187  0.01818705  0.01207972 -0.0274413 ]\n",
            "\n",
            "Phrase: Les chats adorent dormir au soleil.\n",
            "Embedding (premiers 5 coefficients): [-0.06642637 -0.00122354  0.03399962 -0.00836293  0.02679008]\n",
            "\n",
            "Phrase: Python est un langage de programmation populaire.\n",
            "Embedding (premiers 5 coefficients): [-0.03303327  0.02008152 -0.0155138  -0.06426464 -0.04411807]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction à la base de données vectorielle\n",
        "\n",
        "Une **base de données vectorielle** est un outil qui permet de stocker et de rechercher des *vecteurs* — c’est-à-dire des représentations numériques d’objets comme des phrases, des images ou des documents.  \n",
        "Chaque texte que nous traitons est transformé en un vecteur (*embedding*) qui capture son sens.  \n",
        "En comparant ces vecteurs entre eux, on peut retrouver rapidement les textes les plus proches d’une question donnée.\n",
        "\n",
        "Dans cette fiche, nous allons utiliser **FAISS** ([Facebook AI Similarity Search ](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/)), une bibliothèque optimisée pour ce type de recherche.  \n",
        "FAISS permet de stocker nos embeddings et de retrouver les plus proches en fonction d’une requête.\n"
      ],
      "metadata": {
        "id": "N3JQOwCTZWvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import de la base de donnée vectorielle"
      ],
      "metadata": {
        "id": "ZqluL7XRsriG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q faiss-cpu\n",
        "\n",
        "import faiss\n",
        "\n"
      ],
      "metadata": {
        "id": "yVtiz_XqslVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ajout et recherche d'embeddings dans la BD vectorielle\n",
        "\n",
        "Objectif : construire un mini-index vectoriel avec **FAISS** et réaliser une **recherche par similarité cosinus**.\n",
        "\n",
        "Regardez les exemples de code dans [doc FAISS](https://engineering.fb.com/2017/03/29/data-infrastructure/faiss-a-library-for-efficient-similarity-search/).\n",
        "\n",
        "Nous allons repartir des embeddingsins calculés précédement.\n",
        "\n",
        "* Créez un index FAISS de type `IndexFlatIP`.\n",
        "* Ajoutez vos vecteurs à l’index.\n",
        "* Vérifiez que `index.ntotal` correspond au nombre de phrases insérées.\n",
        "* A partir de la requête en langage naturel (`query`), calculez son embedding, puis interrogez l’index pour obtenir les **k=2** passages les plus proches.\n",
        "* Affichez la requête, puis les phrases retrouvées avec leur score."
      ],
      "metadata": {
        "id": "1Fs2KO74u1T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothèses :\n",
        "# - Vous avez déjà importé FAISS : `import faiss`\n",
        "# - Un modèle d'embedding est disponible dans la variable `model`\n",
        "# - Vous avez déjà calculé les embeddings pour les phrases exemple\n",
        "\n",
        "# TODO: Normalisation L2 des embeddings (cosine via Inner Product)\n",
        "#    - Cette étape est cruciale pour simuler la similarité cosinus\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Création de l'index FAISS (Inner Product)\n",
        "#\n",
        "#  TODO: ajoutez-y les embeddings\n",
        "\n",
        "dim = embeddings.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Encodez-la phrase de query, normalisez-la, puis lancez la recherche top-k\n",
        "\n",
        "query = [\n",
        "    \"Quels animaux aiment se reposer au soleil ?\"\n",
        "]\n",
        "\n",
        "\n",
        "# Affichage des résultats\n",
        "#    - Montrez la requête\n",
        "#    - Pour chaque (indice, score) retourné, affichez la phrase correspondante\n",
        "\n"
      ],
      "metadata": {
        "id": "xvwQQpbedDA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chargement d’un fichier PDF\n",
        "\n",
        "Dans cette cellule, deux fonctions sont fournies : elles permettent de charger un fichier texte ou un fichier PDF et de renvoyer une chaîne de caractères contenant le texte extrait.\n"
      ],
      "metadata": {
        "id": "ujH8-eBazQ-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pathlib\n",
        "\n",
        "# Étape 1 : upload du fichier\n",
        "uploaded = files.upload()\n",
        "file_path = list(uploaded.keys())[0]  # premier fichier uploadé\n",
        "ext = pathlib.Path(file_path).suffix.lower()\n",
        "\n",
        "# Étape 2 : fonctions de lecture\n",
        "def read_txt(path: str) -> str:\n",
        "    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def read_pdf(path: str) -> str:\n",
        "    from PyPDF2 import PdfReader\n",
        "    reader = PdfReader(path)\n",
        "    pages_text = []\n",
        "    for p in reader.pages:\n",
        "        t = p.extract_text() or \"\"\n",
        "        pages_text.append(t)\n",
        "    return \"\\n\".join(pages_text)\n",
        "\n",
        "# Étape 3 : extraction du texte\n",
        "if ext == \".txt\":\n",
        "    full_text = read_txt(file_path)\n",
        "elif ext == \".pdf\":\n",
        "    !pip install -q PyPDF2\n",
        "    full_text = read_pdf(file_path)\n",
        "else:\n",
        "    raise ValueError(\"Format non supporté. Utilisez un .txt ou un .pdf\")\n",
        "\n",
        "print(f\"Longueur du texte : {len(full_text)} caractères\")\n",
        "print(\"Aperçu (500 premiers caractères) :\\n\")\n",
        "print(full_text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "n4lbdpHn19rN",
        "outputId": "8f4012bd-8a5e-41f2-f1da-4db77673e6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c852b0e8-5974-41d5-8a3a-0bf703bdd22b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c852b0e8-5974-41d5-8a3a-0bf703bdd22b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Rapport final Comité expert IA programmes 20250115.pdf to Rapport final Comité expert IA programmes 20250115 (1).pdf\n",
            "Longueur du texte : 78115 caractères\n",
            "Aperçu (500 premiers caractères) :\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            "       \n",
            " \n",
            "      \n",
            "   \n",
            "Comité expert  \n",
            "15 janvier 2025  \n",
            "Adapter les programmes d’études \n",
            "pour tenir compte de  l’IA  \n",
            "à l’Université de Sherbrooke  \n",
            "Recomm andations  et invitation à l’action  \n",
            " \n",
            " \n",
            "Rapport  du comité expert  | page 1   \n",
            " \n",
            "Composition du Comité  \n",
            " \n",
            " Professeur  Dany  Baillargeon , Faculté des lettres et sciences humaines   \n",
            " Professeur  Daniel  Chamberland- Tremblay , École de gestion   \n",
            " Professeur  Jean -François  Desbiens, vice-doyen, Faculté des sciences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chunking\n",
        "\n",
        "Lorsqu’on travaille avec de longs documents, il n’est pas possible de calculer directement un embedding unique pour l’ensemble du texte : cela dépasserait les limites des modèles et produirait des vecteurs trop généraux.  \n",
        "La solution est de **découper le document en morceaux plus petits (chunks)**, chaque morceau recevant son propre embedding.  \n",
        "\n",
        "Il existe de nombreuses stratégies de découpage (par phrases, par paragraphes, avec chevauchement, etc.).  \n",
        "Dans cet exercice, nous allons utiliser une approche **naïve** : découper le texte en blocs de taille fixe, éventuellement avec un petit chevauchement pour ne pas couper trop brutalement les idées.  \n",
        "\n",
        "**TODO** Implémentez la fonction décrite dans la cellule suivante pour réaliser ce chunking simple.\n",
        "Testez-là"
      ],
      "metadata": {
        "id": "sG00DRmuwIFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemple de document (hard-coded)\n",
        "document_text = (\n",
        "    \"L’intelligence artificielle (IA) désigne un ensemble de méthodes permettant à des systèmes \"\n",
        "    \"d’exécuter des tâches qui requièrent habituellement l’intelligence humaine : perception, \"\n",
        "    \"raisonnement, apprentissage, langage. Dans les pipelines RAG, on découpe les documents en \"\n",
        "    \"fragments (chunks) de taille contrôlée afin d’optimiser le rappel et la précision lors de la \"\n",
        "    \"recherche vectorielle. Un chevauchement (overlap) entre chunks limite les coupures brutales \"\n",
        "    \"de contexte et améliore la pertinence des passages récupérés.\"\n",
        ")\n",
        "\n",
        "def chunk_text(text: str, chunk_size: int = 300, overlap: int = 50):\n",
        "    \"\"\"\n",
        "    Découpe naïve par caractères avec chevauchement.\n",
        "    - chunk_size : taille cible d’un chunk (en caractères)\n",
        "    - overlap    : nombre de caractères de recouvrement entre deux chunks consécutifs\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "nLTKhev2hanq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test du chunking\n",
        "\n",
        "Testez d'abord sur le court texte proposé. Ensuite, chargez un document de votre choix, et observez la découpe de celui-ci. Qu'en pensez-vous ?\n",
        "\n"
      ],
      "metadata": {
        "id": "ZK4Nb8xnyFmy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test du chunking\n",
        "\n",
        "# Paramètres de découpage\n",
        "chunk_size = 500\n",
        "overlap = 50"
      ],
      "metadata": {
        "id": "Nqu37nrsy-Dh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pause réflexive sur le chunking\n",
        "\n",
        "### 1. Problème du découpage naïf\n",
        "\n",
        "* Quels sont les risques si l’on découpe simplement un document en blocs de longueur fixe (par ex. 500 caractères) ?\n",
        "* Donnez un exemple de perte de sens ou de rupture logique que cela pourrait causer.\n",
        "\n",
        "Voici un liens vers des techniques de chunking plus évoluées: [https://www.pinecone.io/learn/chunking-strategies](https://www.pinecone.io/learn/chunking-strategies) lisez cet article et répondez aux questions suivantes.\n",
        "\n",
        "### 2. Méthodes proposées\n",
        "\n",
        "L’article décrit plusieurs stratégies de chunking.\n",
        "Pour chacune, résumez en **2–3 phrases** l’idée principale :\n",
        "\n",
        "* **Fixed-size chunking** (découpage naïf).\n",
        "* **Overlap chunking** (chevauchement).\n",
        "* **Semantic chunking** (découpage selon le sens).\n",
        "* **Recursive chunking** (utilisation de séparateurs hiérarchiques, comme paragraphes, phrases, mots).\n",
        "\n",
        "\n",
        "\n",
        "### 3. Comparaison\n",
        "\n",
        "* Quelle est selon vous la **principale limite** de la méthode *overlap chunking* ?\n",
        "* En quoi le *semantic chunking* peut-il produire de meilleurs résultats que les autres méthodes ?\n",
        "* Quel inconvénient peut poser ce type de méthode plus avancée ?\n"
      ],
      "metadata": {
        "id": "TN8m1b5d66fl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indexation du document\n",
        "\n",
        "Nous allons tout de même durant cette fiche pousuivre avec le chunking \"naïf\".\n",
        "\n",
        "À partir de ce que vous avez construit dans les cellules précédentes, découpez le document en *chunks*, calculez leurs embeddings et ajoutez-les dans la base de données vectorielle.\n"
      ],
      "metadata": {
        "id": "TnXqFVIO6SQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Indexation du document et peuplement de la base de données\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YV-vLx56Uuc",
        "outputId": "cbedc05e-2770-41eb-d196-8739af10db7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunks: 174 | dim=384 | index.ntotal=174\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Construction d’un *prompt enrichi*\n",
        "\n",
        "Jusqu’ici, nous avons appris à découper un document en *chunks*, à calculer leurs embeddings et à les stocker dans une base vectorielle.  \n",
        "L’étape suivante est de construire un **prompt enrichi** : c’est-à-dire de prendre la question posée par l’utilisateur, de rechercher dans la base les passages les plus proches, puis de **concaténer ces extraits pertinents** avec la question.  \n",
        "Ce prompt enrichi sera ensuite envoyé au modèle de langage, qui pourra répondre en s’appuyant sur le contenu du document plutôt que seulement sur sa mémoire interne.\n",
        "\n",
        "La fonction `build_rag_prompt` que vous allez implémenter doit donc :\n",
        "1. Prendre en entrée une question utilisateur (`query`).  \n",
        "2. Rechercher les *k* chunks les plus pertinents dans l’index FAISS.  \n",
        "3. Filtrer éventuellement selon un score minimal (`min_score`).  \n",
        "4. Limiter la taille totale du contexte (`max_context_chars`).  \n",
        "5. Construire un texte final contenant :  \n",
        "   - un en-tête de contexte avec les extraits retenus,  \n",
        "   - un en-tête pour la question utilisateur.  \n",
        "\n",
        "\n",
        "Pour une question :\n",
        "\n",
        "```text\n",
        "Quels animaux aiment se reposer au soleil ?\n",
        "```\n",
        "\n",
        "Et une base de données vectorielle qui contient les embeddings des phrases suivantes :\n",
        "\n",
        "* « Les chats aiment dormir au soleil. »\n",
        "* « Les chiens sont de loyaux compagnons. »\n",
        "* « L’intelligence artificielle transforme l’éducation. »\n",
        "* « Python est un langage de programmation populaire. »\n",
        "\n",
        "Le *prompt enrichi* pourrait ressembler à ceci :\n",
        "\n",
        "```text\n",
        "Contexte (extraits pertinents) :\n",
        "Les chats aiment dormir au soleil.\n",
        "\n",
        "Question :\n",
        "Quels animaux aiment se reposer au soleil ?\n",
        "\n",
        "Instruction: Réponds de manière factuelle et concise, en t'appuyant uniquement sur le contexte.\n",
        "```\n",
        "\n",
        "Vous allez maintenant compléter l’implémentation de la fonction `build_rag_prompt` afin de générer ce type de structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "RUl0TDlv7V0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Cette cellule suppose que les variables suivantes existent déjà (depuis la 1ère cellule) :\n",
        "# - full_text (texte extrait)\n",
        "# - chunks (liste de morceaux de texte)\n",
        "# - model (SentenceTransformer chargé)\n",
        "# - embeddings (np.ndarray float32, normalisé L2)\n",
        "# - index (faiss.IndexFlatIP construit et rempli)\n",
        "\n",
        "import faiss\n",
        "from typing import List, Tuple\n",
        "\n",
        "def build_rag_prompt(\n",
        "    query: str,\n",
        "    k: int = 3,\n",
        "    min_score: float = 0.0,\n",
        "    max_context_chars: int = 2000,\n",
        "    context_header: str = \"Contexte (extraits pertinents) :\",\n",
        "    question_header: str = \"Question :\",\n",
        "    separator: str = \"\\n---\\n\",\n",
        ") -> Tuple[str, List[Tuple[str, float, int]]]:\n",
        "    \"\"\"\n",
        "    Recherche les k chunks les plus proches de la requête et construit un prompt.\n",
        "    Paramètres:\n",
        "      - query : question utilisateur\n",
        "      - k : nombre de chunks à insérer\n",
        "      - min_score : seuil de similarité (cosine simulée via inner product après normalisation)\n",
        "      - max_context_chars : limite de longueur totale pour le contexte concaténé\n",
        "      - context_header, question_header : en-têtes\n",
        "      - separator : séparateur entre extraits\n",
        "    Retourne:\n",
        "      - prompt (str)\n",
        "      - hits : liste (chunk_text, score, chunk_index)\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "WC6ntg26k8vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test de l'enrichissement\n",
        "\n",
        "Avec un pdf ou un document texte (pas trop court) de votre choix, poser une question pour générer un prompts enrichi. Injecter celui-ci dans votre chatbot favori (à la main) pour voir la réponse."
      ],
      "metadata": {
        "id": "aT3v8Zut7iRU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calcul de la similarité\n",
        "\n",
        "Pour bien comprendre ce que représente la notion de similarité, veuillez prendre connaissance du contenu du colab [suivant](https://colab.research.google.com/drive/1G6FeB5Q1uaM5AUaxNa1zKIs-kTIqkRdC?usp=sharing).\n",
        "\n",
        "Dans celui-ci vous trouverez un explication de différentes distance possibles et quels sont leur avantages relatifs.\n",
        "\n"
      ],
      "metadata": {
        "id": "5gYuFGFf3aas"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Réécriture du RAG sans FAISS\n",
        "\n",
        "Jusqu’à présent, nous avons utilisé **FAISS** pour rechercher les passages les plus proches d’une requête dans la base vectorielle.  \n",
        "Pour cette dernière étape, vous allez recréer cette partie du pipeline **sans utiliser FAISS**.\n",
        "\n",
        "Vous devez écrire votre propre fonction qui compare l’embedding d’une requête avec ceux de la base, calcule une mesure de similarité ou de distance, et retourne les résultats les plus proches.  \n",
        "\n",
        "Utilisez la mesure de proximité la plus pertinente.\n",
        "\n",
        "**Attendu :**  \n",
        "- une fonction claire et autonome,  \n",
        "- un appel qui retourne les *k* passages les plus proches pour une requête donnée,  \n",
        "- un affichage qui montre les phrases retenues et leur score de similarité.\n",
        "\n",
        "Vous ne devez pas refaire l'enrichissement du prompt.\n",
        "\n",
        "Le but de l’exercice est de comprendre ce que fait FAISS « sous le capot », en implémentant vous-même la mécanique de base.\n"
      ],
      "metadata": {
        "id": "prUaToSi84Aw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mini retrieval"
      ],
      "metadata": {
        "id": "Kyl8w5i49DY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxldsE7F5H_X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}